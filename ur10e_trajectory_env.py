"""
UR10e Trajectory Tracking Environment with Task-Space RRT* Integration

Advanced RL environment that combines Task-Space RRT* global planning with
local RL control for precise trajectory tracking. Replaces traditional PID
controllers with learned torque control.

Features:
- Task-Space RRT* planning in 3D Cartesian space
- 19D observation space with relative position and progress
- OI-style reward function for trajectory tracking
- Joint-specific action scaling and momentum inhibition
- Stable-Baselines3 compatibility
"""

# Isaac Gym imports MUST be before PyTorch imports
try:
    # Check if already imported to avoid Foundation object conflicts
    import sys
    if 'isaacgym.gymapi' in sys.modules:
        # Use existing imports
        gymapi = sys.modules['isaacgym.gymapi']
        # Import missing modules if needed
        if 'isaacgym.gymtorch' not in sys.modules:
            from isaacgym import gymtorch
        else:
            gymtorch = sys.modules['isaacgym.gymtorch']
        if 'isaacgym.gymutil' not in sys.modules:
            from isaacgym import gymutil
        else:
            gymutil = sys.modules['isaacgym.gymutil']
    else:
        from isaacgym import gymapi
        from isaacgym import gymtorch
        from isaacgym import gymutil
    from isaacgym.torch_utils import *
    print("‚úÖ Isaac Gym imported successfully in ur10e_trajectory_env")
except (ImportError, KeyError) as e:
    print(f"‚ùå Failed to import Isaac Gym in ur10e_trajectory_env: {e}")
    # Don't sys.exit here, let the main script handle the error

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import torch
import math
from typing import Dict, Any, Tuple, Optional, List
import yaml
from collections import deque
import warnings

# Local imports
from ur10e_incremental_env import UR10eIncrementalEnv
from task_space_planner import TaskSpacePlannerInterface, TSPlanningRequest, TSWaypoint

# Suppress specific warnings for cleaner output
warnings.filterwarnings("ignore", category=UserWarning)


class UR10eTrajectoryEnv(UR10eIncrementalEnv):
    """
    UR10e Environment with Task-Space RRT* + RL trajectory tracking

    Extends the base UR10eIncrementalEnv with:
    - Task-Space global planning
    - 19D observation space with trajectory information
    - OI-style trajectory tracking rewards
    - Waypoint progression management
    """

    def __init__(self, config_path: str = "config.yaml", num_envs: int = 1, mode: str = "trajectory_tracking"):
        """
        Initialize the trajectory tracking environment

        Args:
            config_path: Path to configuration file
            num_envs: Number of parallel environments
            mode: "point_to_point" or "trajectory_tracking"
        """
        # Initialize base environment first
        super().__init__(config_path, num_envs)

        # Trajectory tracking configuration--
        self.mode: str = None        # ÂàùÂßãÂåñ‰∏∫ NoneÔºåÂêéÈù¢Áî® set_mode ËÆæÂÆö
        self.ts_planner = None       # ÂÖàÂç†‰∏Ä‰∏™Â±ûÊÄßÔºåÈÅøÂÖç hasattr ÈóÆÈ¢ò
        self.trajectory_config = self.config.get('trajectory_tracking', {})
        self.task_space_config = self.config.get('task_space', {})
        self.ts_rrt_config = self.config.get('ts_rrt_star', {})

        # Trajectory tracking state
        # Trajectory tracking state (vectorized)
        self.current_ts_waypoints = []
        self.current_waypoint_index = np.zeros(self.num_envs, dtype=np.int32)
        self.trajectory_completed = np.zeros(self.num_envs, dtype=bool)


        # Reward function parameters (MUST be set before planner initialization)
        self.waypoint_threshold = self.trajectory_config.get('waypoint_threshold', 0.15)
        self.waypoint_bonus = self.trajectory_config.get('waypoint_bonus', 5.0)
        self.smooth_coef = self.trajectory_config.get('smooth_coef', 0.1)
        self.use_deviation_penalty = self.trajectory_config.get('use_deviation_penalty', False)
        self.deviation_coef = self.trajectory_config.get('deviation_coef', 2.0)

        # ÂàùÂßãÂåñËßÑÂàíÂô®ÔºàÂ¶ÇÊûúÂàùÂßã mode ÈúÄË¶ÅÔºâ+ ËÆæÁΩÆ mode
        self.set_mode(mode)

        # Initialize Task-Space planner for trajectory tracking mode
        if self.mode == "trajectory_tracking":
            self._init_task_space_planner()

        # Override observation space always (19D)
        self._define_observation_space_19d()

        print(f"‚úÖ UR10eTrajectoryEnv initialized:")
        print(f"   üéØ Control Mode: {self.mode}")
        print(f"   üé¨ Device: {self.device}")
        print(f"   üîß Parallel envs: {num_envs}")
        print(f"   üìä Observation space: {self.observation_space}")
        print(f"   üìê Action space: {self.action_space}")
        print(f"   üõ§Ô∏è  Task-Space planner: {'‚úÖ' if self.mode == 'trajectory_tracking' else '‚ùå'}")

    def _init_task_space_planner(self):
        workspace_bounds_list = []
        for axis in ['x', 'y', 'z']:
            bounds = self.task_space_config.get('workspace_bounds', {}).get(axis, [-0.5, 0.5])
            workspace_bounds_list.append(bounds)
        workspace_bounds = np.array(workspace_bounds_list)

        # ‚úÖ ‰∏∫ÊØè‰∏™ env ÂàõÂª∫‰∏Ä‰∏™ plannerÔºàÂêÑËá™Áª¥Êä§ waypoint_indexÔºâ
        self.ts_planners = [
            TaskSpacePlannerInterface(
                workspace_bounds=workspace_bounds,
                waypoint_spacing=0.1,
                replanning_threshold=self.ts_rrt_config.get('replanning_threshold', 0.1),
                max_waypoints=self.ts_rrt_config.get('max_waypoints', 50)
            )
            for _ in range(self.num_envs)
        ]
        self.ts_planner = self.ts_planners[0]  # ÂÖºÂÆπÊóßÂºïÁî®

        print(f"   üó∫Ô∏è  Workspace bounds: {workspace_bounds}")
        print(f"   üìè Waypoint threshold: {self.waypoint_threshold}m")


    def set_mode(self, mode: str):
        """
        ÂàáÊç¢ÁéØÂ¢ÉÊ®°ÂºèÔºö
        - "trajectory_tracking": ÂêØÁî®‰ªªÂä°Á©∫Èó¥ËΩ®ËøπËßÑÂàí + ËΩ®ËøπÂ•ñÂä±
        - "point_to_point": ‰ΩøÁî®Âü∫Á°ÄÁéØÂ¢ÉÁöÑÁÇπÂØπÁÇπÂ•ñÂä±

        Á∫¶ÊùüÔºö
        - Âè™ËÉΩÂú® episode ‰πãÈó¥Ë∞ÉÁî®ÔºàÂç≥ reset ÂâçÂêéÔºâÔºå‰∏çË¶ÅÂú®Âçï‰∏™ episode ‰∏≠ÈÄîÊç¢„ÄÇ
        - ËßÇÊµãÁª¥Â∫¶Âõ∫ÂÆö‰∏∫ 19DÔºåÊú¨ÂáΩÊï∞‰∏ç‰ºö‰øÆÊîπ observation_space„ÄÇ
        """
        assert mode in ["trajectory_tracking", "point_to_point"], \
            f"Unsupported mode: {mode}"

        if self.mode == mode:
            return  # ‰∏çÈúÄË¶ÅÈáçÂ§çÂàáÊç¢

        self.mode = mode

        if mode == "trajectory_tracking":
            # Á°Æ‰øù‰ªªÂä°Á©∫Èó¥ËßÑÂàíÂô®Â∑≤ÂàùÂßãÂåñ
            if self.ts_planner is None:
                self._init_task_space_planner()
        else:
            # ÂàáÂõû point_to_point Ê®°ÂºèÔºö
            # Ê∏ÖÁ©∫ÂΩìÂâçËΩ®Ëøπ
            self.current_ts_waypoints = []
            self.current_waypoint_index[:] = 0
            self.trajectory_completed[:] = False

        print(f"üîÅ Switched UR10eTrajectoryEnv mode to: {self.mode}")

    def _define_observation_space_19d(self):
        """
        Define 19D observation space for trajectory tracking:

        19D = [joint_pos(6) + joint_vel(6) + delta_to_waypoint(3) + progress(1) + tcp_pos(3)]

        Key insight: Use relative position (delta) instead of absolute positions
        for better alignment with reward function and generalization.
        """
        obs_dim = 19  # Trajectory tracking: joints + velocities + delta + progress + tcp_pos
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(obs_dim,),
            dtype=np.float32
        )

        print(f"üéØ Trajectory observation space: {obs_dim}D")
        print(f"   Structure: [joint_pos(6) + joint_vel(6) + delta_to_waypoint(3) + progress(1) + tcp_pos(3)]")

    def plan_trajectory(self, start_tcp: np.ndarray, goal_tcp: np.ndarray) -> bool:
        """
        Plan trajectory from start TCP to goal TCP using Task-Space RRT*

        Args:
            start_tcp: Starting TCP position [x, y, z]
            goal_tcp: Goal TCP position [x, y, z]

        Returns:
            True if planning successful, False otherwise
        """
        if self.mode != "trajectory_tracking":
            print("‚ö†Ô∏è Trajectory planning not available in point_to_point mode")
            return False

        print(f"üõ§Ô∏è  Planning trajectory from {start_tcp} to {goal_tcp}")

        # Create planning request
        request = TSPlanningRequest(
            start_tcp=start_tcp,
            target_tcp=goal_tcp,
            max_planning_time=5.0,
            tolerance=self.waypoint_threshold
        )

        # Plan trajectory
        result = self.ts_planner.plan_to_target(request)

        if result.success:
            self.current_ts_waypoints = result.waypoints
            self.current_waypoint_index[:] = 0
            self.trajectory_completed[:] = False

            for p in self.ts_planners:
                p.current_waypoints = result.waypoints
                p.current_waypoint_index = 0

            print(f"‚úÖ Trajectory planned: {len(self.current_ts_waypoints)} waypoints")
            return True
        else:
            print(f"‚ùå Trajectory planning failed: {result.error_message}")
            self.current_ts_waypoints = []
            self.current_waypoint_index[:] = 0
            return False

    def set_waypoints(self, waypoints: List[TSWaypoint]):
        """
        Set waypoints directly (for testing or external planning)

        Args:
            waypoints: List of TSWaypoint objects
        """
        self.current_ts_waypoints = waypoints
        self.current_waypoint_index[:] = 0
        self.trajectory_completed[:] = False
        print(f"üìç Set {len(waypoints)} waypoints for trajectory tracking")

    def get_current_waypoint_(self) -> Optional[TSWaypoint]:
        """Get current waypoint for trajectory tracking"""
        if not self.current_ts_waypoints or self.current_waypoint_index >= len(self.current_ts_waypoints):
            return None
        return self.current_ts_waypoints[self.current_waypoint_index]
    
    def get_current_waypoint(self, env_id: int = 0) -> Optional[TSWaypoint]:
        """Get current waypoint for a specific env (default env0 for logging/callback)."""
        if self.mode != "trajectory_tracking":
            return None
        if not self.current_ts_waypoints:
            return None

        # ÊØè‰∏™ env ÊúâËá™Â∑±ÁöÑ planner ÂíåÊ†áÈáè index
        planner = self.ts_planners[env_id] if hasattr(self, "ts_planners") else self.ts_planner
        if planner.current_waypoints is None or len(planner.current_waypoints) == 0:
            return None

        idx = int(planner.current_waypoint_index)
        if idx >= len(planner.current_waypoints):
            return None
        return planner.current_waypoints[idx]

    
    def get_observation(self):
        obs_list = []
        for i in range(self.num_envs):
            q = self.joint_positions[i].detach().cpu().numpy()
            qd = self.joint_velocities[i].detach().cpu().numpy()
            tcp = self._forward_kinematics(self.joint_positions[i]).detach().cpu().numpy()

            if self.mode == "trajectory_tracking" and self.current_ts_waypoints:
                wp = self.ts_planners[i].get_current_waypoint()
                if wp is not None:
                    wp_pos = np.asarray(wp.cartesian_position, np.float32)
                    delta = wp_pos - tcp
                    prog = self.ts_planners[i].current_waypoint_index / max(1, len(self.current_ts_waypoints)-1)
                else:
                    delta = np.zeros(3, np.float32)
                    prog = 1.0
            else:
                target = self.target_positions[i].detach().cpu().numpy()
                delta = target - tcp
                prog = 0.0

            obs_i = np.concatenate([q, qd, delta, np.array([prog], np.float32), tcp]).astype(np.float32)
            obs_list.append(obs_i)

        return obs_list[0] if self.num_envs == 1 else np.stack(obs_list, axis=0)
    
    def _trajectory_reward(self, tcp_pos, action_tensor, env_id: int = 0):
        planner = self.ts_planners[env_id]
        waypoint = planner.get_current_waypoint()
        if waypoint is None:
            return -10.0, False

        waypoint_pos = torch.tensor(waypoint.cartesian_position, device=tcp_pos.device)
        distance = torch.norm(tcp_pos - waypoint_pos)

        reward = -distance.item()
        waypoint_reached = distance.item() < waypoint.tolerance
        if waypoint_reached:
            reward += self.waypoint_bonus

        # Âπ≥ÊªëÈ°π/ÂÅèÁ¶ªÈ°πÂ¶ÇÊûúÂêØÁî®Ôºå‰πüÁî® planner ÁöÑ index
        if self.use_deviation_penalty and len(planner.current_waypoints) > 1:
            reward -= self.deviation_coef * self._calculate_path_deviation(tcp_pos, env_id)

        reward -= self.smooth_coef * torch.norm(action_tensor).item()
        return reward, waypoint_reached


    def _calculate_path_deviation(self, tcp_pos: torch.Tensor,env_id: int = 0) -> float:
        """
        Calculate deviation from planned path (simplified line segment distance)

        Args:
            tcp_pos: Current TCP position

        Returns:
            Deviation distance
        """
        planner = self.ts_planners[env_id]
        idx = planner.current_waypoint_index
        if idx <= 0 or idx >= len(planner.current_waypoints):
            return 0.0
        current_wp = planner.current_waypoints[idx-1]
        next_wp = planner.current_waypoints[idx]

        # Simple deviation: distance from line segment between current and next waypoint
        current_wp_pos = torch.tensor(current_wp.cartesian_position, device=tcp_pos.device)
        next_wp_pos = torch.tensor(next_wp.cartesian_position, device=tcp_pos.device)

        # Project onto line segment and calculate perpendicular distance
        line_vec = next_wp_pos - current_wp_pos
        point_vec = tcp_pos - current_wp_pos

        line_len = torch.norm(line_vec)
        if line_len < 1e-6:
            return torch.norm(point_vec).item()

        line_unitvec = line_vec / line_len
        proj_length = torch.dot(point_vec, line_unitvec).clamp(0, line_len)
        proj_point = current_wp_pos + proj_length * line_unitvec

        deviation = torch.norm(tcp_pos - proj_point)
        return deviation.item()
    
    def step(self, action: np.ndarray):
        """
        Step the environment with trajectory tracking support (vectorized).

        Args:
            action: 6D normalized action array [-1, 1]
                    shape can be (6,) for single env or (num_envs, 6) for multi env

        Returns:
            (obs, reward, terminated, truncated, info)
            - single env: reward float, terminated bool, truncated bool, info dict
            - multi env: reward (num_envs,), terminated (num_envs,), truncated (num_envs,), info list[dict]
        """
        # --------- debug: action change ----------
        if hasattr(self, "_last_action"):
            try:
                action_change = np.linalg.norm(action - self._last_action)
                # print(f"üîÑ Action change magnitude: {action_change:.6f}")
            except Exception:
                pass
        self._last_action = action.copy()

        # --------- record joint positions before ----------
        joint_pos_before = self.joint_positions.clone()

        # --------- physics step from parent ----------
        obs_base, reward_base, terminated_base, truncated_base, info_base = super().step(action)

        # --------- record joint positions after ----------
        joint_pos_after = self.joint_positions
        try:
            joint_change = torch.norm(joint_pos_after - joint_pos_before, dim=1).mean().item()
            # print(f"üîß Joint position change(mean): {joint_change:.6f}")
        except Exception:
            pass

        # --------- normalize terminated/truncated to bool vectors ----------
        def _to_bool_vec(x):
            if x is None:
                return np.zeros(self.num_envs, dtype=bool)
            if isinstance(x, (bool, np.bool_)):
                return np.full(self.num_envs, bool(x), dtype=bool)
            if torch.is_tensor(x):
                x = x.detach().cpu().numpy()
            x = np.asarray(x).reshape(-1)
            if x.size == 1:
                return np.full(self.num_envs, bool(x[0]), dtype=bool)
            return x.astype(bool)

        term_base = _to_bool_vec(terminated_base)
        trunc_base = _to_bool_vec(truncated_base)

        # --------- trajectory tracking mode ----------
        if self.mode == "trajectory_tracking":
            # action tensor to device
            action_tensor = torch.as_tensor(action, dtype=torch.float32, device=self.device)
            if action_tensor.ndim == 1:
                action_tensor = action_tensor.unsqueeze(0)  # (1, 6)

            rewards = np.zeros(self.num_envs, dtype=np.float32)
            term_traj = np.zeros(self.num_envs, dtype=bool)
            infos = []

            for i in range(self.num_envs):
                # 1) TCP per env
                current_tcp = self._forward_kinematics(self.joint_positions[i])

                # debug: tcp change per env
                if not hasattr(self, "_last_tcp_list"):
                    self._last_tcp_list = [None for _ in range(self.num_envs)]
                last_tcp = self._last_tcp_list[i]
                if last_tcp is not None:
                    try:
                        tcp_change = torch.norm(current_tcp - last_tcp).item()
                        # print(f"[env {i}] üìç TCP change: {tcp_change:.6f}")
                    except Exception:
                        pass
                self._last_tcp_list[i] = current_tcp.detach().clone()

                # 2) update planner progress per env
                advanced = self.ts_planners[i].update_progress(current_tcp.detach().cpu().numpy())
                if advanced:
                    self.current_waypoint_index[i] = self.ts_planners[i].current_waypoint_index
                    print(f"üìç [env {i}] Waypoint {self.current_waypoint_index[i] + 1}/{len(self.current_ts_waypoints)} reached")

                # 3) reward per env
                r_i, wp_reached_i = self._trajectory_reward(current_tcp, action_tensor[i], env_id=i)
                rewards[i] = r_i

                # 4) completion check per env
                current_wp = self.ts_planners[i].get_current_waypoint()
                if current_wp is None and len(self.current_ts_waypoints) > 0:
                    self.trajectory_completed[i] = True
                    term_traj[i] = True
                    print(f"üéâ [env {i}] Trajectory completed successfully!")
                elif current_wp is not None and self.current_waypoint_index[i] == len(self.current_ts_waypoints) - 1:
                    final_dist = torch.norm(
                        current_tcp - torch.tensor(current_wp.cartesian_position, device=self.device)
                    )
                    if final_dist < current_wp.tolerance:
                        self.trajectory_completed[i] = True
                        term_traj[i] = True
                        print(f"üéâ [env {i}] Trajectory completed successfully!")

                # 5) info per env
                infos.append({
                    "trajectory_mode": True,
                    "current_waypoint": int(self.current_waypoint_index[i]),
                    "total_waypoints": len(self.current_ts_waypoints),
                    "waypoint_reached": bool(wp_reached_i),
                    "trajectory_completed": bool(self.trajectory_completed[i]),
                    "distance_to_waypoint": float(
                        self.ts_planners[i].get_distance_to_current_waypoint(current_tcp.detach().cpu().numpy())
                    )
                })

            obs = self.get_observation()
            terminated_out = np.logical_or(term_base, term_traj)
            truncated_out = trunc_base

            if self.num_envs == 1:
                return obs, float(rewards[0]), bool(terminated_out[0]), bool(truncated_out[0]), infos[0]
            return obs, rewards, terminated_out, truncated_out, infos

        # --------- point-to-point / base mode ----------
        else:
            # ËøôÈáåÁõ¥Êé•Áî®Áà∂Á±ª rewardÔºàÂÆÉÊú¨Ë∫´Â∞±ÊòØÂêëÈáèÂåñÁöÑÔºâ
            reward_out = reward_base
            if torch.is_tensor(reward_out):
                reward_out = reward_out.detach().cpu().numpy()
            reward_out = np.asarray(reward_out).reshape(-1)
            if reward_out.size == 1:
                reward_out = np.full(self.num_envs, float(reward_out[0]), dtype=np.float32)

            if self.num_envs == 1:
                return obs_base, float(reward_out[0]), bool(term_base[0]), bool(trunc_base[0]), info_base
            # info_base Â¶ÇÊûúÊòØ dictÔºåÂ∞±ÂπøÊí≠Êàê list[dict]
            if isinstance(info_base, dict):
                info_base = [info_base.copy() for _ in range(self.num_envs)]
            return obs_base, reward_out, term_base, trunc_base, info_base


    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, Dict]:
        """Reset environment for new episode"""

        # 1) Â¶ÇÊûú options ÈáåÊåáÂÆö‰∫Ü modeÔºåÂ∞±ÂÖàÂàáÊç¢Ôºà‰øùÁïô‰Ω†ÂéüÊù•ÁöÑÈÄªËæëÔºâ
        if options and "mode" in options:
            self.set_mode(options["mode"])

        # 2) ÂÖà reset Â∫ïÂ±ÇÂ¢ûÈáèÂäõÁü©ÁéØÂ¢É
        obs, info = super().reset(seed=seed, options=options)

        # 3) ÈáçÁΩÆËΩ®ËøπË∑üË∏™Áä∂ÊÄÅ
        self.current_waypoint_index[:] = 0
        self.trajectory_completed[:] = False
        
        # Âà†Èô§_prev_distance_to_waypointÂèòÈáèÔºåÁ°Æ‰øùÊØèÊ¨°resetÈÉΩÈáçÊñ∞ÂºÄÂßã
        if hasattr(self, '_prev_distance_to_waypoint'):
            delattr(self, '_prev_distance_to_waypoint')
            
        # Âà†Èô§Ë∞ÉËØïÁî®ÁöÑÂèòÈáè
        if hasattr(self, '_last_action'):
            delattr(self, '_last_action')
        if hasattr(self, '_last_tcp'):
            delattr(self, '_last_tcp')

        planned = False

        # 4) Â¶ÇÊûúÂú® trajectory_tracking Ê®°ÂºèÔºå‰ºòÂÖàÁúã options ÈáåÊòØÂê¶ÊòæÂºèÁªô‰∫Ü start/goal
        if self.mode == "trajectory_tracking":
            if options is not None and "plan_trajectory" in options:
                plan_options = options["plan_trajectory"]
                if "start_tcp" in plan_options and "goal_tcp" in plan_options:
                    planned = self.plan_trajectory(
                        np.array(plan_options["start_tcp"], dtype=np.float32),
                        np.array(plan_options["goal_tcp"], dtype=np.float32),
                    )

            # 5) Â¶ÇÊûúÊ≤°ÊúâÈÄöËøá options ËßÑÂàíÊàêÂäüÔºåÂ∞±Ëá™Âä®ÈááÊ†∑‰∏Ä‰∏™ËΩ®Ëøπ
            if not planned:
                # ÂΩìÂâç TCP ‰Ωú‰∏∫Ëµ∑ÁÇπ
                with torch.no_grad():
                    start_tcp = (
                        self._forward_kinematics(self.joint_positions[0])
                        .cpu()
                        .numpy()
                    )

                # ‰ªé task_space_config ÈáåËØª workspace_bounds
                ws_cfg = self.task_space_config.get("workspace_bounds", {})
                def _axis(name, default):
                    return ws_cfg.get(name, default)

                goal_tcp = np.array(
                    [
                        np.random.uniform(*_axis("x", [-0.6, 0.6])),
                        np.random.uniform(*_axis("y", [-0.6, 0.6])),
                        np.random.uniform(*_axis("z", [0.2, 0.8])),
                    ],
                    dtype=np.float32,
                )

                planned = self.plan_trajectory(start_tcp, goal_tcp)

                if not planned:
                    print("‚ö†Ô∏è Auto trajectory planning failed in reset()")
                else:
                    print(
                        f"üîÅ New episode trajectory planned: "
                        f"{len(self.current_ts_waypoints)} waypoints"
                    )

        # 6) ÁîüÊàêËßÇÊµãÔºà19DÔºöjoint_pos + joint_vel + delta_to_waypoint + progress + tcp_posÔºâ
        obs = self.get_observation()

        # 7) Ë°•ÂÖÖ info
        """info.update(
            {
                "trajectory_mode": self.mode == "trajectory_tracking",
                "trajectory_completed": False,
                "current_waypoint": 0,
                "total_waypoints": len(self.current_ts_waypoints)
                if self.mode == "trajectory_tracking"
                else 0,
            }
        )"""
        infos = []
        for i in range(self.num_envs):
            infos.append({
                "trajectory_mode": self.mode == "trajectory_tracking",
                "trajectory_completed": False,
                "current_waypoint": int(self.current_waypoint_index[i]),
                "total_waypoints": len(self.current_ts_waypoints) if self.mode=="trajectory_tracking" else 0
            })

        #return obs, info
        return (obs, infos[0]) if self.num_envs == 1 else (obs, infos)
    
    def get_trajectory_statistics(self) -> Dict[str, Any]:
        """Get trajectory tracking statistics"""
        if self.mode != "trajectory_tracking":
            return {'trajectory_mode': False}

        return {
            'trajectory_mode': True,
            'current_waypoint': self.current_waypoint_index + 1,
            'total_waypoints': len(self.current_ts_waypoints),
            'progress_percentage': self.ts_planner.get_planning_progress()['progress_percentage'],
            'planner_stats': self.ts_planner.get_statistics(),
            'trajectory_completed': self.trajectory_completed
        }


def test_trajectory_environment():
    """Test UR10e trajectory tracking environment"""
    print("üß™ Testing UR10e Trajectory Tracking Environment")

    # Create environment in trajectory tracking mode
    env = UR10eTrajectoryEnv(config_path="config.yaml", mode="trajectory_tracking")

    # Reset environment
    obs, info = env.reset()
    print(f"üìä Initial observation shape: {obs.shape}")
    print(f"üéØ Environment info: {info}")

    # Plan a trajectory
    start_tcp = np.array([0.5, 0.0, 0.3])
    goal_tcp = np.array([-0.3, 0.4, 0.7])

    if env.plan_trajectory(start_tcp, goal_tcp):
        print("‚úÖ Trajectory planned successfully")

        # Test waypoint information
        current_waypoint = env.get_current_waypoint()
        print(f"   Current waypoint: {current_waypoint}")

        # Test observation structure
        obs = env.get_observation()
        print(f"   Observation shape: {obs.shape}")
        print(f"   First 6 values (joint pos): {obs[:6]}")
        print(f"   Next 6 values (joint vel): {obs[6:12]}")
        print(f"   Next 3 values (delta to waypoint): {obs[12:15]}")
        print(f"   Progress value: {obs[15]}")
        print(f"   Last 3 values (TCP pos): {obs[16:19]}")

        # Test a few steps
        for step in range(5):
            action = env.action_space.sample()  # Random action
            obs, reward, terminated, truncated, info = env.step(action)

            print(f"Step {step + 1}: reward={reward:.3f}, waypoint={info.get('current_waypoint', 0)}")

            if terminated:
                print("Episode completed!")
                break
    else:
        print("‚ùå Trajectory planning failed")

    # Close environment
    env.close()
    print("‚úÖ Trajectory environment test completed")


if __name__ == "__main__":
    test_trajectory_environment()